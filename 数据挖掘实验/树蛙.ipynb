{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow.keras as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Flatten,Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label\\tcomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0\\t环境很惬意，菜很好吃！价格比较合理！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0\\t超级差！服务差！配送差！口味差！点的东西带不完！直接来一句钱退你好了！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0\\t感觉还是挺不错的，很好喝，然后，嗯，其实也没有便宜太多，和原价相比一块钱左右吧！哦，就...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\t一如既往的实惠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0\\t味道不错，挺好吃的，分量也足，关键是佐料打得好，看起都安逸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0\\t囗味细，味道好。性价比高。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0\\t味道不错，馅也多，不想有些地方根本吃不到馅，推荐。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0\\t服务态度很好，烧烤味道很好吃。而去好划算！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0\\t亲  味道真不好吃，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0\\t不是毛血旺么.咋是一盆豆芽呢.从来不写评价的我实在忍不住了.本来饿死了.现在没胃口了....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         label\\tcomment\n",
       "0                                 0\\t环境很惬意，菜很好吃！价格比较合理！\n",
       "1                0\\t超级差！服务差！配送差！口味差！点的东西带不完！直接来一句钱退你好了！\n",
       "2     0\\t感觉还是挺不错的，很好喝，然后，嗯，其实也没有便宜太多，和原价相比一块钱左右吧！哦，就...\n",
       "3                                            0\\t一如既往的实惠\n",
       "4                      0\\t味道不错，挺好吃的，分量也足，关键是佐料打得好，看起都安逸\n",
       "...                                                 ...\n",
       "9995                                   0\\t囗味细，味道好。性价比高。\n",
       "9996                       0\\t味道不错，馅也多，不想有些地方根本吃不到馅，推荐。\n",
       "9997                           0\\t服务态度很好，烧烤味道很好吃。而去好划算！\n",
       "9998                                      0\\t亲  味道真不好吃，\n",
       "9999  0\\t不是毛血旺么.咋是一盆豆芽呢.从来不写评价的我实在忍不住了.本来饿死了.现在没胃口了....\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('train.csv',encoding = 'utf-8')\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "x=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "     x.append(df['label\\tcomment'][i].split('\\t')[1])\n",
    "     x_train.append(' '.join(jieba.cut(x[i],cut_all=False)))\n",
    "     y_train.append(df['label\\tcomment'][i].split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = \"哈工大停用词表.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_x=[]\n",
    "for i in range(len(x_train)):\n",
    "    q=x_train[i].split()\n",
    "    strr=''\n",
    "    for word in q:\n",
    "        if word not in stopwords:\n",
    "            strr=strr+word+' '\n",
    "    fil_x.append(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y=np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words =12000   #设置的最大词数\n",
    "max_len=50\n",
    "\n",
    "tk = Tokenizer(num_words=num_words+1)  #因为要加未登录词，所以+1\n",
    "\n",
    "tk.fit_on_texts(fil_x)\n",
    "tk.word_index[tk.oov_token] = num_words + 1 \n",
    "sequence=tk.texts_to_sequences(fil_x)\n",
    "data_x=pad_sequences(sequence,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data_x, data_y, random_state=1,train_size=0.8,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.read_csv('test_new.csv',encoding = 'utf-8')\n",
    "id=[]\n",
    "x_test=[]\n",
    "x1=[]\n",
    "for i in range(len(dff)):\n",
    "     x1.append(dff['comment'][i])\n",
    "     id.append(dff['id'][i])\n",
    "     x_test.append(' '.join(jieba.cut(x1[i],cut_all=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_x_test=[]\n",
    "for i in range(len(x_test)):\n",
    "    q=x_test[i].split()\n",
    "    strr=''\n",
    "    for word in q:\n",
    "        if word not in stopwords:\n",
    "            strr=strr+word+' '\n",
    "    fil_x_test.append(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1=tk.texts_to_sequences(fil_x_test)\n",
    "data_x_test=pad_sequences(sequence1,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-29be4e829a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#output_dim=60\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(num_words,120,input_length=max_len))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=60\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history=model.fit(X_train,Y_train,epochs=20,batch_size=64,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict=history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict=history.history\n",
    "loss_values=history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "epochs=range(1,len(loss_values)+1)\n",
    "plt.plot(epochs,loss_values,'bo',label='Training Loss')\n",
    "plt.plot(epochs,val_loss_values,'b',label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc=history_dict['acc']\n",
    "val_acc=history_dict['val_acc']\n",
    "plt.plot(epochs,acc,'bo',label=\"Training acc\")\n",
    "plt.plot(epochs,val_acc,label='Validation acc')\n",
    "plt.title(\"Traing and Validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(num_words,120,input_length=max_len))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=60\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(X_train,Y_train,epochs=8,batch_size=64,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.evaluate(X_test,Y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1=model.predict(data_x_test)\n",
    "output1=[]\n",
    "for i in range(len(features1)):\n",
    "    if features1[i][0]>0.5 or features1[i][0]==0.5:\n",
    "        output1.append(1)\n",
    "    else :\n",
    "        output1.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df3=pd.read_csv(\"correct.csv\",encoding='utf-8')\n",
    "df3=df3['label']\n",
    "df3array=df3.values\n",
    "df3array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN =0\n",
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for i in range(len(output1)):\n",
    "    if (output1[i]==0)and(int(df3array[i])==0):\n",
    "        TN=TN+1\n",
    "    if (output1[i]==1)and(int(df3array[i])==1):\n",
    "        TP=TP+1\n",
    "    if (output1[i]==0)and(int(df3array[i])==1):\n",
    "        FN=FN+1\n",
    "    if (output1[i]==1)and(int(df3array[i])==0):\n",
    "        FP=FP+1;\n",
    "acc=(TP/(TP+FP))\n",
    "callbackrate=TP/(TP+FN)\n",
    "score=(2*acc*callbackrate)/(acc+callbackrate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id))\n",
    "print(len(output1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameToSave=pd.DataFrame({'id':id,'label':output1})\n",
    "dataFrameToSave.shape\n",
    "dataFrameToSave.to_csv(\"sample.csv\",index=False,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameToSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3.6] *",
   "language": "python",
   "name": "conda-env-Python3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
