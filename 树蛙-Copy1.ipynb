{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Flatten,Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv',encoding = 'utf-8')\n",
    "df=df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "x=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "     x.append(df['label\\tcomment'][i].split('\\t')[1])\n",
    "     x_train.append(' '.join(jieba.cut(x[i],cut_all=False)))\n",
    "     y_train.append(df['label\\tcomment'][i].split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = \"哈工大停用词表.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_x=[]\n",
    "for i in range(len(x_train)):\n",
    "    q=x_train[i].split()\n",
    "    strr=''\n",
    "    for word in q:\n",
    "        if word not in stopwords:\n",
    "            strr=strr+word+' '\n",
    "    fil_x.append(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y=np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words =12000   #设置的最大词数\n",
    "max_len=80\n",
    "\n",
    "tk = Tokenizer(num_words=num_words+1)  #因为要加未登录词，所以+1\n",
    "\n",
    "tk.fit_on_texts(fil_x)\n",
    "tk.word_index[tk.oov_token] = num_words + 1 \n",
    "sequence=tk.texts_to_sequences(fil_x)\n",
    "data_x=pad_sequences(sequence,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data_x, data_y, random_state=1,train_size=0.9,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.read_csv('test_new.csv',encoding = 'utf-8')\n",
    "id=[]\n",
    "x_test=[]\n",
    "x1=[]\n",
    "for i in range(len(dff)):\n",
    "     x1.append(dff['comment'][i])\n",
    "     id.append(dff['id'][i])\n",
    "     x_test.append(' '.join(jieba.cut(x1[i],cut_all=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_x_test=[]\n",
    "for i in range(len(x_test)):\n",
    "    q=x_test[i].split()\n",
    "    strr=''\n",
    "    for word in q:\n",
    "        if word not in stopwords:\n",
    "            strr=strr+word+' '\n",
    "    fil_x_test.append(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1=tk.texts_to_sequences(fil_x_test)\n",
    "data_x_test=pad_sequences(sequence1,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "zh_model = KeyedVectors.load_word2vec_format('zh.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(len(zh_model[next(iter(zh_model.vocab))]))\n",
    "embedding_dim = len(zh_model[next(iter(zh_model.vocab))])\n",
    "embedding_matrix = np.random.rand(num_words, embedding_dim)\n",
    "embedding_matrix = (embedding_matrix - 0.5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1ade85f8c37d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1ade85f8c37d>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=200\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(num_words,embedding_dim)\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=200\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history=model.fit(X_train,Y_train,epochs=20,batch_size=520,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=model.evaluate(X_test,Y_test)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim))\n",
    "model.add(LSTM(units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "result10=model.evaluate(X_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model=Sequential()\n",
    "model.add(Embedding(num_words,120,input_length=max_len))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=60\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history=model.fit(X_train,Y_train,epochs=20,batch_size=1024,validation_data=(X_test,Y_test))\n",
    "result1=model.evaluate(X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict=history.history\n",
    "loss_values=history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "epochs=range(1,len(loss_values)+1)\n",
    "plt.plot(epochs,loss_values,'bo',label='Training Loss')\n",
    "plt.plot(epochs,val_loss_values,'b',label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc=history_dict['acc']\n",
    "val_acc=history_dict['val_acc']\n",
    "plt.plot(epochs,acc,'bo',label=\"Training acc\")\n",
    "plt.plot(epochs,val_acc,label='Validation acc')\n",
    "plt.title(\"Traing and Validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(num_words,120,input_length=max_len))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=60\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(X_train,Y_train,epochs=15,batch_size=1024,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.evaluate(X_test,Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1=model.predict(data_x_test)\n",
    "output1=[]\n",
    "for i in range(len(features1)):\n",
    "    if features1[i][0]>0.5 or features1[i][0]==0.5:\n",
    "        output1.append(1)\n",
    "    else :\n",
    "        output1.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(num_words,120,input_length=max_len))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50,dropout=0.1,recurrent_dropout=0.5)))#output_dim=60\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(X_train,Y_train,epochs=15,batch_size=1024,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.evaluate(X_test,Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features5=model.predict(data_x_test)\n",
    "output5=[]\n",
    "for i in range(len(features5)):\n",
    "    if features5[i][0]>0.5 or features5[i][0]==0.5:\n",
    "        output5.append(1)\n",
    "    else :\n",
    "        output5.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df3=pd.read_csv(\"correct.csv\",encoding='utf-8',sep=',')\n",
    "df3=df3['label']\n",
    "df3array=df3.values\n",
    "df3array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=0\n",
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for i in range(len(output1)):\n",
    "    if (output1[i]==0)and(int(df3array[i])==0):\n",
    "        TN=TN+1\n",
    "    if (output1[i]==1)and(int(df3array[i])==1):\n",
    "        TP=TP+1\n",
    "    if (output1[i]==0)and(int(df3array[i])==1):\n",
    "        FN=FN+1\n",
    "    if (output1[i]==1)and(int(df3array[i])==0):\n",
    "        FP=FP+1;\n",
    "acc=(TP/(TP+FP))\n",
    "callbackrate=TP/(TP+FN)\n",
    "score=(2*acc*callbackrate)/(acc+callbackrate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id))\n",
    "print(len(output1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameToSave=pd.DataFrame({'id':id,'label':output1})\n",
    "dataFrameToSave.shape\n",
    "dataFrameToSave.to_csv(\"sample.csv\",index=False,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameToSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
